{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54706956",
   "metadata": {},
   "source": [
    "# Simulations for Clustered Switchback Experiments\n",
    "<a id='toc'></a>\n",
    "\n",
    "## Table of Contents \n",
    "[Section 1: Model](#sec1)\n",
    "\n",
    "[Section 2: Step by Step Guide](#step_by_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9c4116f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current dir: /Users/cleeyu/Documents/GitHub/spatiotemporal_graph_cluster_randomization\n"
     ]
    }
   ],
   "source": [
    "import time, importlib, sys, os, pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set up directory\n",
    "current_dir = os.getcwd() # Get current directory\n",
    "print(\"current dir:\", current_dir)\n",
    "\n",
    "parent_dir = os.path.dirname(current_dir) # Go up by one level\n",
    "sys.path.append(parent_dir) # Add parent dir to sys path\n",
    "\n",
    "from helpers import utils, graph_helpers, mdp_helpers, stats_helpers, print_nicely"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141be9b8",
   "metadata": {},
   "source": [
    "## 1. Model <a id='sec1'></a> \n",
    "We will use a linear reward function\n",
    "$$\\mu_{it}(S_{it}): = E[Y_{it} \\mid S_{it}, W] = b_{it} + a_{it} \\frac {S_{it}} m.$$\n",
    "We devide by $m$ to normalize. The state evolves as a clipped random walk with states $\\{0,1,...,m\\}$. Specifially, define *competition level* as the proportion of neighbors assigned arm $1$, formally,  \n",
    "$$C_{it} = \\frac{\\sum_{j\\in {\\cal N}(i)\\backslash i}W_{jt}}{|{\\cal N}(i)\\backslash i|}.$$\n",
    "The probability of going up conditional on non-lazy event is \n",
    "$$P^{\\rm up}_{it} = \\sigma(\\alpha_{it} + \\beta_{it}W_{it} - \\gamma_{it} C_{it})$$\n",
    "(expect at the ceiling), where $\\sigma$ is the sigmod function.\n",
    "Accounting for the laziness $L\\in [0,1]$, the probability of going up is then \n",
    "$$(1-L)\\cdot P^{\\rm up}_{it}.$$\n",
    "Assume that $\\gamma_{it} \\leq \\beta_{it}$ such that under full treatment the net effect is still positive.\n",
    "For a given $\\delta$, the exposure mapping is computed as the joint of $W_{it} = a$ AND at least $(1-\\delta)$ fraction of its spatiotemporal neighbors excluding $(i,t)$ itself have treatment equal to $a$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0478606",
   "metadata": {},
   "source": [
    "<a id='step_by_step'></a> \n",
    "\n",
    "## 2. Step-by-step guide\n",
    "\n",
    "We now combine the components in the above sections.\n",
    "Pipeline:\n",
    "1. Define instance (interference graph, reward func, transition probability)\n",
    "2. Generate treatment assignment W\n",
    "3. Run the MDP induced by W\n",
    "4. estimate ATE using HT estimator\n",
    "\n",
    "Back to [Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6d8ed86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_config = {\n",
    "    \"T\": 1000, # time horizon\n",
    "    \"num_states\": 3, # maximum inventory\n",
    "    'time_block_length': 4, # time blocks used for randomized design\n",
    "    \"recency\": 3, # how many rounds to look back in HT/Hajek estimator \n",
    "    \"delta\": 0.05, #  expo map X_ita^r = 1 if more than 1-delta spatio-temp neighbors are assigned arm a\n",
    "    'num_W_for_computing_prop': 10**5, # num of trtmt asgn mtx used for finding prop score (monte carlo) \n",
    "    'initial_state': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae1a145",
   "metadata": {},
   "source": [
    "## 2.1: Setup interference graph and spatial clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "337421cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(graph_helpers)\n",
    "importlib.reload(stats_helpers)\n",
    "\n",
    "########## STRICT LATTICE GRAPH ##########\n",
    "\n",
    "sim_config.update({\n",
    "    \"n\": 1, # if constructing a strict lattice graph, this quantity must be a perfect square\n",
    "    'num_cells_per_dim': 1, # defines spatial block for randomized design\n",
    "    \"kappa\": 3, # two nodes interfere (thru reward and transition) if L1 dist <= kappa \n",
    "})\n",
    "\n",
    "# create interference graph\n",
    "adj_matrix = graph_helpers.generate_interference_graph_from_lattice(\n",
    "    sqrt_n=int(np.sqrt(sim_config['n'])), \n",
    "    kappa=sim_config['kappa']\n",
    "    )\n",
    "print(np.mean(np.sum(adj_matrix,axis=1)))\n",
    "\n",
    "# setup spatial clusters\n",
    "cluster_matrix = graph_helpers.generate_clusters_from_lattice(sqrt_n=int(np.sqrt(sim_config['n'])), num_cells_per_dim=sim_config['num_cells_per_dim'])\n",
    "adj_clusters = np.matmul(adj_matrix,cluster_matrix) \n",
    "print(np.mean(np.sum(adj_clusters,axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "87491dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importlib.reload(graph_helpers)\n",
    "# importlib.reload(stats_helpers)\n",
    "\n",
    "# ########## SPATIAL GRAPH WITH UNIFORM COORDS ##########\n",
    "\n",
    "# sim_config.update({\n",
    "#     \"n\": 3600, \n",
    "#     'num_cells_per_dim': 2, # defines spatial partition of unit square used for randomized design\n",
    "#     \"kappa\": 0.025, # two nodes interfere (thru reward and transition) if Euclidean dist between coordinates <= kappa \n",
    "# })\n",
    "\n",
    "# # create interference graph\n",
    "# coords_array = graph_helpers.generate_random_points(sim_config['n'])\n",
    "# adj_matrix = graph_helpers.build_adjacency_matrix_from_coords(coords_array, sim_config['kappa'])\n",
    "# print(np.mean(np.sum(adj_matrix,axis=1)))\n",
    "\n",
    "# # setup spatial clusters\n",
    "# cluster_matrix = graph_helpers.spatial_clustering_map(coords_array, sim_config['num_cells_per_dim'])\n",
    "# adj_clusters = np.matmul(adj_matrix,cluster_matrix) \n",
    "# print(np.mean(np.sum(adj_clusters,axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc156973",
   "metadata": {},
   "source": [
    "## 2.2: Initialize MC and find true ATE using Monte Carlo \n",
    "Back to [Table of Contents](#toc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "26261f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(mdp_helpers)\n",
    "\n",
    "# initialize Markov chain model\n",
    "\n",
    "MC = mdp_helpers.InventoryMarkovChain(\n",
    "    max_inventory=sim_config['num_states']-1,\n",
    "    adj_matrix=adj_matrix,\n",
    "    num_rounds=sim_config['T'],\n",
    "    C_lazy = 0.1,\n",
    "    C_alpha = -5,\n",
    "    C_beta = 10,\n",
    "    C_gamma = 0,\n",
    "    C_baseline = 100,\n",
    "    C_slope= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "97a9b28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== True ATE (apx'ed using Monte Carlo) ====================\n",
      "Mean reward under all-1 vs. all-0: 109.9655 vs. 100.0346  \n",
      "True ATE: 9.9309\n",
      "==================== True ATE (apx'ed using Monte Carlo) ====================\n",
      "Mean reward under all-1 vs. all-0: 109.1296 vs. 100.8701  \n",
      "ATE Est (Diff): 8.2595\n"
     ]
    }
   ],
   "source": [
    "# approximate ATE via Monte Carlo\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "num_sims_apx_ate = 10**5\n",
    "initial_state = sim_config['initial_state']\n",
    "\n",
    "sim_results_0 = MC.simulate_MC(initial_state * np.ones((sim_config['n'])), np.zeros((sim_config['n'], sim_config['T'],num_sims_apx_ate)), use_sigmoid=True)\n",
    "sim_results_1 = MC.simulate_MC(initial_state * np.ones((sim_config['n'])), np.ones((sim_config['n'], sim_config['T'],num_sims_apx_ate)), use_sigmoid=True)\n",
    "\n",
    "all_0_mean = np.mean(sim_results_0[\"rewards\"])\n",
    "all_1_mean = np.mean(sim_results_1[\"rewards\"])\n",
    "\n",
    "print(\"=\"*20 + \" True ATE (apx'ed using Monte Carlo) \" + \"=\"*20)\n",
    "print(f\"Mean reward under all-1 vs. all-0: {all_1_mean:.4f} vs. {all_0_mean:.4f}  \")\n",
    "true_ATE = all_1_mean - all_0_mean\n",
    "print(f\"True ATE: {true_ATE:.4f}\")\n",
    "\n",
    "# CHECK #####################\n",
    "time_cluster_matrix_check = stats_helpers.generate_time_blocks(T=sim_config['T'], time_block_length=sim_config['time_block_length'])\n",
    "W_ATE_check = stats_helpers.generate_cluster_treatments(cluster_matrix,time_cluster_matrix_check,num_sims_apx_ate)\n",
    "temp_1 = MC.simulate_MC(initial_state * np.ones((sim_config['n'])), W_ATE_check, use_sigmoid=True)\n",
    "temp_0 = MC.simulate_MC(initial_state * np.ones((sim_config['n'])), 1-W_ATE_check, use_sigmoid=True)\n",
    "sim_results_1_check = temp_1[\"rewards\"] * W_ATE_check + temp_0[\"rewards\"] * (1-W_ATE_check)\n",
    "sim_results_0_check = temp_0[\"rewards\"] * W_ATE_check + temp_1[\"rewards\"] * (1-W_ATE_check)\n",
    "\n",
    "all_1_mean_check = np.mean(temp_1[\"rewards\"] * W_ATE_check + temp_0[\"rewards\"] * (1-W_ATE_check))\n",
    "all_0_mean_check = np.mean(temp_0[\"rewards\"] * W_ATE_check + temp_1[\"rewards\"] * (1-W_ATE_check))\n",
    "\n",
    "print(\"=\"*20 + \" True ATE (apx'ed using Monte Carlo) \" + \"=\"*20)\n",
    "print(f\"Mean reward under all-1 vs. all-0: {all_1_mean_check:.4f} vs. {all_0_mean_check:.4f}  \")\n",
    "ATE_check = all_1_mean_check - all_0_mean_check\n",
    "print(f\"ATE Est (Diff): {ATE_check:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c85775",
   "metadata": {},
   "source": [
    "## 2.3: Compute propensity scores\n",
    "Back to [Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "52cb5d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.994\n",
      "1.747\n",
      "(1, 1000, 100000)\n",
      "minimum of emp prop score: 0.24548, 0.24639\n",
      "total number of zeros: 0, 0\n",
      "mean emp prop score (interior units): 0.3132, 0.3133\n",
      "expected #(i,t) with X_it=1 is 313.22, 313.29\n",
      "%nz in emp prop_score_0, prop_score_1: 100.00%, 100.00%\n",
      "Printed at: 2026-02-11 13:01:43 EST\n",
      "3.4936753802853477\n",
      "0.8696715383955588\n",
      "4.0586062746053\n",
      "1.9829859802891192\n",
      "3.4943608692725823\n",
      "0.8697028215250228\n",
      "4.073651621313346\n",
      "1.9832219423675705\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(stats_helpers)\n",
    "\n",
    "time_cluster_matrix = stats_helpers.generate_time_blocks(T=sim_config['T'], time_block_length=sim_config['time_block_length'])\n",
    "time_adj_matrix = np.tril(np.ones((sim_config['T'],sim_config['T'])), k=0) - np.tril(np.ones((sim_config['T'],sim_config['T'])), k=-(sim_config['recency'] + 1))\n",
    "\n",
    "print(np.mean(np.sum(time_adj_matrix,axis=1)))\n",
    "print(np.mean(np.sum(time_adj_matrix @ time_cluster_matrix > 0,axis=1)))\n",
    "\n",
    "# Compute propensity score (Monte Carlo)\n",
    "\n",
    "arms_tensor = stats_helpers.generate_cluster_treatments(cluster_matrix,time_cluster_matrix,num_W=sim_config['num_W_for_computing_prop'])\n",
    "print(arms_tensor.shape)\n",
    "\n",
    "emp_prop_score_results = stats_helpers.empirical_propensity_scores(arms_tensor,adj_matrix,time_adj_matrix,sim_config['delta'])\n",
    "propensity_1_array, propensity_0_array = emp_prop_score_results['propensity_1'], emp_prop_score_results['propensity_0']\n",
    "\n",
    "print(f\"minimum of emp prop score: {np.amin(propensity_1_array)}, {np.amin(propensity_0_array)}\")\n",
    "print(f\"total number of zeros: {np.count_nonzero(propensity_1_array == 0)}, {np.count_nonzero(propensity_0_array == 0)}\")\n",
    "\n",
    "# Display emp_prop_score_results\n",
    "prop_1_mean, prop_0_mean = propensity_1_array.mean(), propensity_0_array.mean()\n",
    "print(f\"mean emp prop score (interior units): {prop_1_mean:.4f}, {prop_0_mean:.4f}\") \n",
    "n, T = sim_config['n'], sim_config['T']\n",
    "print(f\"expected #(i,t) with X_it=1 is {n*T*prop_1_mean:.2f}, {n*T*prop_0_mean:.2f}\")\n",
    "print(f\"%nz in emp prop_score_0, prop_score_1: {len(np.nonzero(propensity_0_array)[0])/(n*T)*100:.2f}%, {len(np.nonzero(propensity_1_array)[0])/(n*T)*100:.2f}%\")\n",
    "utils.print_time()\n",
    "\n",
    "HT_weights_0 = np.zeros((np.shape(propensity_0_array)))\n",
    "np.divide(1, propensity_0_array, out = HT_weights_0, where=propensity_0_array != 0)\n",
    "print(np.mean(HT_weights_0))\n",
    "print(np.std(HT_weights_0))\n",
    "print(np.amax(HT_weights_0))\n",
    "print(np.amin(HT_weights_0))\n",
    "\n",
    "HT_weights_1 = np.zeros((np.shape(propensity_1_array)))\n",
    "np.divide(1, propensity_1_array, out = HT_weights_1, where=propensity_1_array != 0)\n",
    "print(np.mean(HT_weights_1))\n",
    "print(np.std(HT_weights_1))\n",
    "print(np.amax(HT_weights_1))\n",
    "print(np.amin(HT_weights_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd52978a",
   "metadata": {},
   "source": [
    "## 2.4: Simulate experiment and compute HT and Hajek estimators\n",
    "Back to [Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "41101548",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(stats_helpers)\n",
    "\n",
    "num_iter_sim = 10**5\n",
    "\n",
    "initial_state = sim_config['initial_state']\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "W = stats_helpers.generate_cluster_treatments(cluster_matrix,time_cluster_matrix,num_iter_sim)\n",
    "exposure_results = stats_helpers.exposure_mapping(W, adj_matrix, time_adj_matrix, sim_config['delta'])\n",
    "\n",
    "sim_results = MC.simulate_MC(initial_state * np.ones((sim_config['n'])), W, use_sigmoid=True)\n",
    "rewards = sim_results[\"rewards\"]\n",
    "\n",
    "# rewards_0 = rewards[W == 0]\n",
    "# rewards_1 = rewards[W == 1]\n",
    "# print(np.mean(rewards_0))\n",
    "# print(np.std(rewards_0))\n",
    "# print(np.amax(rewards_0))\n",
    "# print(np.amin(rewards_0))\n",
    "# print(np.mean(rewards_1))\n",
    "# print(np.std(rewards_1))\n",
    "# print(np.amax(rewards_1))\n",
    "# print(np.amin(rewards_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1aec0aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Fake Reward Horvitz-Thompson Estimates ====================\n",
      "mean_HT_est: 9.9883\n",
      "bias: 0.0574\n",
      "var_HT_est: 540.2065\n",
      "\n",
      "==================== Vanilla Horvitz-Thompson Estimates ====================\n",
      "mean_HT_est: 8.2789\n",
      "bias: -1.6520\n",
      "var_HT_est: 176.7988\n"
     ]
    }
   ],
   "source": [
    "# CHECK with deteriministic rewards set to be all_0_mean for all control units and all_1_mean for all treated units\n",
    "ht_results = stats_helpers.horvitz_thompson(all_0_mean * (W == 0) + all_1_mean * (W == 1),exposure_results['exposure_1'],exposure_results['exposure_0'],propensity_1_array,propensity_0_array)\n",
    "ate_estimate_ht = ht_results['ate_estimate_ht']\n",
    "\n",
    "print(\"\\n\" + \"=\"*20 + \" Fake Reward Horvitz-Thompson Estimates \" + \"=\"*20)\n",
    "mean_HT_est, var_HT_est = ate_estimate_ht.mean(), ate_estimate_ht.var()\n",
    "print(f\"mean_HT_est: {mean_HT_est:.4f}\\nbias: {mean_HT_est - true_ATE:.4f}\\n\" + f\"var_HT_est: {var_HT_est:.4f}\")\n",
    "\n",
    "# CHECK with the vanilla HT, using exposure mapping as W and 1-W, and ground truth 0.5 propensity scores\n",
    "ht_results = stats_helpers.horvitz_thompson(rewards,W,1-W,0.5*np.ones(propensity_1_array.shape),0.5*np.ones(propensity_0_array.shape))\n",
    "ate_estimate_ht = ht_results['ate_estimate_ht']\n",
    "\n",
    "print(\"\\n\" + \"=\"*20 + \" Vanilla Horvitz-Thompson Estimates \" + \"=\"*20)\n",
    "mean_HT_est, var_HT_est = ate_estimate_ht.mean(), ate_estimate_ht.var()\n",
    "print(f\"mean_HT_est: {mean_HT_est:.4f}\\nbias: {mean_HT_est - true_ATE:.4f}\\n\" + f\"var_HT_est: {var_HT_est:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "38bb05c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Horvitz-Thompson Estimates ====================\n",
      "mean_HT_est: 9.9785\n",
      "bias: 0.0476\n",
      "var_HT_est: 540.1955\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(stats_helpers)\n",
    "\n",
    "#ht_results = stats_helpers.horvitz_thompson(sim_results_0[\"rewards\"] * (W == 0) + sim_results_1[\"rewards\"] * (W == 1),W,1-W,propensity_1_array,propensity_0_array)\n",
    "#ht_results = stats_helpers.horvitz_thompson(sim_results_0_check * (1-W) + sim_results_1_check * W,W,1-W,propensity_1_array,propensity_0_array)\n",
    "#ht_results = stats_helpers.horvitz_thompson(sim_results_0_check * (1-W) + sim_results_1_check * W,exposure_results['exposure_1'],exposure_results['exposure_0'],propensity_1_array,propensity_0_array)\n",
    "ht_results = stats_helpers.horvitz_thompson(rewards,exposure_results['exposure_1'],exposure_results['exposure_0'],propensity_1_array,propensity_0_array)\n",
    "ate_estimate_ht = ht_results['ate_estimate_ht']\n",
    "\n",
    "print(\"\\n\" + \"=\"*20 + \" Horvitz-Thompson Estimates \" + \"=\"*20)\n",
    "mean_HT_est, var_HT_est = ate_estimate_ht.mean(), ate_estimate_ht.var()\n",
    "print(f\"mean_HT_est: {mean_HT_est:.4f}\\nbias: {mean_HT_est - true_ATE:.4f}\\n\" + f\"var_HT_est: {var_HT_est:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9468480d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Hajek Estimates ====================\n",
      "mean_HT_est: 9.9209\n",
      "bias: -0.0100\n",
      "var_Hajek_est: 0.0017\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(stats_helpers)\n",
    "\n",
    "#hajek_results = stats_helpers.hajek(all_0_mean * (W == 0) + all_1_mean * (W == 1),exposure_results['exposure_1'],exposure_results['exposure_0'],propensity_1_array,propensity_0_array)\n",
    "#hajek_results = stats_helpers.hajek(temp_1[\"rewards\"],exposure_results['exposure_1'],exposure_results['exposure_0'],propensity_1_array,propensity_0_array)\n",
    "hajek_results = stats_helpers.hajek(rewards,exposure_results['exposure_1'],exposure_results['exposure_0'],propensity_1_array,propensity_0_array)\n",
    "ate_estimate_hajek = hajek_results['ate_estimate_hajek']\n",
    "\n",
    "print(\"\\n\" + \"=\"*20 + \" Hajek Estimates \" + \"=\"*20)\n",
    "mean_Hajek_est, var_Hajek_est = ate_estimate_hajek.mean(), ate_estimate_hajek.var()\n",
    "print(f\"mean_HT_est: {mean_Hajek_est:.4f}\\nbias: {mean_Hajek_est - true_ATE:.4f}\\n\" + f\"var_Hajek_est: {var_Hajek_est:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
