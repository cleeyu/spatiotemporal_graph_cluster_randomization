{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a34f2eea",
   "metadata": {},
   "source": [
    "# Load Dataset\n",
    "\n",
    "We scale the x,y coordinates to be within the unit square.\n",
    "For entries that did not have capacity, we randomly sample without replacement from the distribution of reported capacitites.\n",
    "\n",
    "Before the coordinate transformation, a distance of ~540 is roughly 10 min walk. (what units are these coordinates? I estimated this by just picking two hotels and looking at google maps)\n",
    "\n",
    "DC Office of the Chief Technology Officer (OCTO). \"Hotels.\" ArcGIS Hub. Accessed February 17, 2026. https://hub.arcgis.com/datasets/DCGIS::hotels/about. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa3b3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "df = pd.read_csv('Hotels.csv')\n",
    "coord = df[['XCOORD', 'YCOORD']].values\n",
    "capacities = np.array(df[['NUMROOMS']].values).flatten()\n",
    "\n",
    "# scale the x,y coordinates to be within the unit square.\n",
    "scale = max(np.max(coord[:, 0]) - np.min(coord[:, 0]), np.max(coord[:, 1]) - np.min(coord[:, 1]))\n",
    "print(scale)\n",
    "coord[:,0] = (coord[:,0] - np.min(coord[:,0]))/scale\n",
    "coord[:,1] = (coord[:,1] - np.min(coord[:,1]))/scale\n",
    "\n",
    "# For entries that did not have capacity, we randomly sample without replacement from the distribution of reported capacitites\n",
    "permuted_capacities = np.random.permutation(capacities[~np.isnan(capacities)])\n",
    "print(permuted_capacities[0:np.sum(np.isnan(capacities))])\n",
    "capacities[np.isnan(capacities)] = permuted_capacities[0:np.sum(np.isnan(capacities))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3f6f8d",
   "metadata": {},
   "source": [
    "We plot the distribution of the ground truth capacities (including the artificially filled in capacities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66fc2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_arr = np.sort(capacities)\n",
    "plt.plot(sorted_arr)\n",
    "plt.xlabel('Hotels Sorted')\n",
    "plt.ylabel('Capacities')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47693ba",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "We plot the locations of the hotels with the colors given by the capacities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec4b4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = colors.PowerNorm(gamma=0.5, vmin=capacities.min(), vmax=capacities.max())\n",
    "plt.scatter(coord[:,0], coord[:,1], c=capacities, cmap='plasma', norm=norm)\n",
    "plt.colorbar(label='Capacity')\n",
    "plt.xlabel('X Coordinate')\n",
    "plt.ylabel('Y Coordinate')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ffece9",
   "metadata": {},
   "source": [
    "We make the capacities coarser, setting them to ceiling(capacity / 50)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecab1cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "capacities = np.ceil(capacities / 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49c6d0d",
   "metadata": {},
   "source": [
    "## 1. Model <a id='sec1'></a> \n",
    "We will use a linear reward function\n",
    "$$\\mu_{it}(S_{it}): = E[Y_{it} \\mid S_{it}, W] = a_{i} + b_{it} S_{it}.$$\n",
    "We choose $a_{i} \\sim U[0,1]$, and $b_{it} = (1 + 0.2 \\epsilon_{it})/m_i$, where $\\epsilon_{it} \\sim U[0,1]$.\n",
    "\n",
    "The state evolves as a clipped random walk with states $\\{0,1,...,m_i\\}$. Specifially, define *competition level* as the proportion of neighbors assigned arm $1$, formally,  \n",
    "$$C_{it} = \\frac{\\sum_{j\\in {\\cal N}(i)\\backslash i}W_{jt}}{|{\\cal N}(i)\\backslash i|}.$$\n",
    "\n",
    "At each time $t$, with probability $\\lambda^l_{it}$, the state does not change. We choose $\\lambda^l_{it} = 0.1$.\n",
    "\n",
    "Conditioned on the event that the state changes, the departure and arrival rates are governed by parameters $\\lambda^d_{it} \\sim U[0,1]$ and $$\\lambda^a_{it} = \\sigma (\\alpha_{it} + \\beta_{it} W_{it} - \\gamma{it} C_{it}),$$ where $\\sigma(\\cdot)$ denotes the sigmoid function. We choose $\\alpha_{it} \\sim N(-5,2)$, $\\beta_{it} \\sim N(10,4)$, $\\gamma_{it} \\sim U[0,\\beta_{it}/2]$.\n",
    "\n",
    "Conditioned on the event that the state changes, the probability of a departure (state decreasing)is $\\lambda^d_{it}/(\\lambda^a_{it} + \\lambda^d_{it})$, and the probability of an arrival (state increasing) is $\\lambda^a_{it}/(\\lambda^a_{it} + \\lambda^d_{it})$. The states are then clipped to be within $\\{0,1,2, \\dots m_i\\}$.\n",
    "\n",
    "By construction $\\gamma_{it} \\leq \\beta_{it}$ such that under full treatment the net effect is still positive.\n",
    "\n",
    "For a given $\\delta$, the exposure mapping is computed as \n",
    "$$\\prod_{t' = t-r}^{t} \\mathbb{I}\\Big(W_{it} = a, \\sum_{j\\in {\\cal N}(i)\\backslash i}\\mathbb{I}(W_{jt} = a) \\geq (1-\\delta) |{\\cal N}(i)\\backslash i|\\Big).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261f902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, importlib, sys, os, pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set up directory\n",
    "current_dir = os.getcwd() # Get current directory\n",
    "print(\"current dir:\", current_dir)\n",
    "\n",
    "parent_dir = os.path.dirname(current_dir) # Go up by one level\n",
    "sys.path.append(parent_dir) # Add parent dir to sys path\n",
    "\n",
    "from helpers import utils, graph_helpers, mdp_helpers, stats_helpers, print_nicely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461ac944",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_config = {\n",
    "    \"T\": 100, # time horizon\n",
    "    'time_block_length': 10, # length of time blocks used for randomized design\n",
    "    \"recency\": 10, # how many rounds to look back in HT/Hajek estimator \n",
    "    \"delta\": 0.2, #  expo map X_ita^r = 1 if more than 1-delta spatio-temp neighbors are assigned arm a\n",
    "    'num_W_for_computing_prop': 10**4, # num of trtmt asgn mtx used for finding prop score (monte carlo) \n",
    "    'initial_state': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb222fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(graph_helpers)\n",
    "importlib.reload(stats_helpers)\n",
    "\n",
    "########## SPATIAL GRAPH WITH UNIFORM COORDS ##########\n",
    "\n",
    "sim_config.update({\n",
    "    'num_cells_per_dim': 4, # defines spatial partition of unit square used for randomized design\n",
    "    \"kappa\": 0.1, # two nodes interfere (thru reward and transition) if Euclidean dist between coordinates <= kappa \n",
    "})\n",
    "\n",
    "# create interference graph\n",
    "adj_matrix = graph_helpers.build_adjacency_matrix_from_coords(coord, sim_config['kappa'])\n",
    "print(np.mean(np.sum(adj_matrix,axis=1)))\n",
    "\n",
    "# setup spatial clusters\n",
    "cluster_matrix = graph_helpers.spatial_clustering_map(coord, sim_config['num_cells_per_dim'])\n",
    "adj_clusters = np.matmul(adj_matrix,cluster_matrix) \n",
    "print(np.mean(np.sum(adj_clusters,axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf2fcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(mdp_helpers)\n",
    "\n",
    "# initialize Markov chain model\n",
    "num_units = adj_matrix.shape[0]\n",
    "num_rounds = sim_config['T']\n",
    "max_inventory=capacities\n",
    "\n",
    "C_baseline = np.outer(np.random.random(num_units), np.ones(num_rounds))\n",
    "C_slope = np.ones((num_units,num_rounds)) + 0.2 * np.random.random((num_units, num_rounds))\n",
    "C_lazy = 0.1 * np.ones((num_units,num_rounds))\n",
    "C_alpha = np.random.normal(loc = -5, scale = 2, size = ((num_units,num_rounds)))\n",
    "C_beta = np.random.normal(loc = 10, scale = 4, size = ((num_units,num_rounds)))\n",
    "C_gamma = 0.5*np.random.random((num_units,num_rounds)) * C_beta\n",
    "C_depart = np.random.random((num_units,num_rounds))\n",
    "\n",
    "MC = mdp_helpers.InventoryMarkovChain(\n",
    "    max_inventory=max_inventory,\n",
    "    adj_matrix=adj_matrix,\n",
    "    num_rounds=sim_config['T'],\n",
    "    C_baseline = C_baseline,\n",
    "    C_slope= C_slope,\n",
    "    C_lazy = C_lazy,\n",
    "    C_alpha = C_alpha,\n",
    "    C_beta = C_beta,\n",
    "    C_gamma = C_gamma,\n",
    "    C_depart = C_depart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b614f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(mdp_helpers)\n",
    "# approximate ATE via Monte Carlo\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "num_sims_apx_ate = 10**4\n",
    "initial_state = sim_config['initial_state']\n",
    "\n",
    "sim_results_0 = MC.simulate_MC(initial_state * np.ones(num_units), np.zeros((num_units, num_rounds,num_sims_apx_ate)), use_sigmoid=True)\n",
    "sim_results_1 = MC.simulate_MC(initial_state * np.ones(num_units), np.ones((num_units, num_rounds,num_sims_apx_ate)), use_sigmoid=True)\n",
    "\n",
    "all_0_mean = np.mean(sim_results_0[\"rewards\"])\n",
    "all_1_mean = np.mean(sim_results_1[\"rewards\"])\n",
    "\n",
    "print(\"=\"*20 + \" True ATE (apx'ed using Monte Carlo) \" + \"=\"*20)\n",
    "print(f\"Mean reward under all-1 vs. all-0: {all_1_mean:.4f} vs. {all_0_mean:.4f}  \")\n",
    "true_ATE = all_1_mean - all_0_mean\n",
    "print(f\"True ATE: {true_ATE:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6b3965",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(stats_helpers)\n",
    "\n",
    "time_cluster_matrix = stats_helpers.generate_time_blocks(T=sim_config['T'], time_block_length=sim_config['time_block_length'])\n",
    "time_adj_matrix = np.tril(np.ones((sim_config['T'],sim_config['T'])), k=0) - np.tril(np.ones((sim_config['T'],sim_config['T'])), k=-(sim_config['recency'] + 1))\n",
    "\n",
    "print(np.mean(np.sum(time_adj_matrix,axis=1)))\n",
    "print(np.mean(np.sum(time_adj_matrix @ time_cluster_matrix > 0,axis=1)))\n",
    "\n",
    "# Compute propensity score (Monte Carlo)\n",
    "\n",
    "arms_tensor = stats_helpers.generate_cluster_treatments(cluster_matrix,time_cluster_matrix,num_W=sim_config['num_W_for_computing_prop'])\n",
    "print(arms_tensor.shape)\n",
    "\n",
    "emp_prop_score_results = stats_helpers.empirical_propensity_scores(arms_tensor,adj_matrix,time_adj_matrix,sim_config['delta'])\n",
    "propensity_1_array, propensity_0_array = emp_prop_score_results['propensity_1'], emp_prop_score_results['propensity_0']\n",
    "\n",
    "print(f\"minimum of emp prop score: {np.amin(propensity_1_array)}, {np.amin(propensity_0_array)}\")\n",
    "print(f\"total number of zeros: {np.count_nonzero(propensity_1_array == 0)}, {np.count_nonzero(propensity_0_array == 0)}\")\n",
    "\n",
    "# Display emp_prop_score_results\n",
    "prop_1_mean, prop_0_mean = propensity_1_array.mean(), propensity_0_array.mean()\n",
    "print(f\"mean emp prop score (interior units): {prop_1_mean:.4f}, {prop_0_mean:.4f}\") \n",
    "print(f\"expected #(i,t) with X_it=1 is {num_units*num_rounds*prop_1_mean:.2f}, {num_units*num_rounds*prop_0_mean:.2f}\")\n",
    "print(f\"%nz in emp prop_score_0, prop_score_1: {len(np.nonzero(propensity_0_array)[0])/(num_units*num_rounds)*100:.2f}%, {len(np.nonzero(propensity_1_array)[0])/(num_units*num_rounds)*100:.2f}%\")\n",
    "utils.print_time()\n",
    "\n",
    "HT_weights_0 = np.zeros((np.shape(propensity_0_array)))\n",
    "np.divide(1, propensity_0_array, out = HT_weights_0, where=propensity_0_array != 0)\n",
    "print(np.mean(HT_weights_0))\n",
    "print(np.std(HT_weights_0))\n",
    "print(np.amax(HT_weights_0))\n",
    "print(np.amin(HT_weights_0))\n",
    "\n",
    "HT_weights_1 = np.zeros((np.shape(propensity_1_array)))\n",
    "np.divide(1, propensity_1_array, out = HT_weights_1, where=propensity_1_array != 0)\n",
    "print(np.mean(HT_weights_1))\n",
    "print(np.std(HT_weights_1))\n",
    "print(np.amax(HT_weights_1))\n",
    "print(np.amin(HT_weights_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944fc97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(stats_helpers)\n",
    "\n",
    "num_iter_sim = 10**4\n",
    "\n",
    "initial_state = sim_config['initial_state']\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "W = stats_helpers.generate_cluster_treatments(cluster_matrix,time_cluster_matrix,num_iter_sim)\n",
    "exposure_results = stats_helpers.exposure_mapping(W, adj_matrix, time_adj_matrix, sim_config['delta'])\n",
    "\n",
    "sim_results = MC.simulate_MC(initial_state * np.ones(num_units), W, use_sigmoid=True)\n",
    "rewards = sim_results[\"rewards\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd73dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(stats_helpers)\n",
    "\n",
    "ht_results = stats_helpers.horvitz_thompson(rewards,exposure_results['exposure_1'],exposure_results['exposure_0'],propensity_1_array,propensity_0_array)\n",
    "ate_estimate_ht = ht_results['ate_estimate']\n",
    "\n",
    "print(\"\\n\" + \"=\"*20 + \" Horvitz-Thompson Estimates \" + \"=\"*20)\n",
    "mean_HT_est, var_HT_est = ate_estimate_ht.mean(), ate_estimate_ht.var()\n",
    "print(f\"mean_HT_est: {mean_HT_est:.4f}\\nbias: {mean_HT_est - true_ATE:.4f}\\n\" + f\"var_HT_est: {var_HT_est:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b62fcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(stats_helpers)\n",
    "\n",
    "hajek_results = stats_helpers.hajek(rewards,exposure_results['exposure_1'],exposure_results['exposure_0'],propensity_1_array,propensity_0_array)\n",
    "ate_estimate_hajek = hajek_results['ate_estimate']\n",
    "\n",
    "print(\"\\n\" + \"=\"*20 + \" Hajek Estimates \" + \"=\"*20)\n",
    "mean_Hajek_est, var_Hajek_est = ate_estimate_hajek.mean(), ate_estimate_hajek.var()\n",
    "print(f\"mean_HT_est: {mean_Hajek_est:.4f}\\nbias: {mean_Hajek_est - true_ATE:.4f}\\n\" + f\"var_Hajek_est: {var_Hajek_est:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165c1aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(stats_helpers)\n",
    "\n",
    "burn_in = 0\n",
    "DM_results = stats_helpers.diff_means(rewards,W, sim_config['time_block_length'], burn_in)\n",
    "ate_estimate_DM = DM_results['ate_estimate']\n",
    "\n",
    "print(\"\\n\" + \"=\"*20 + \" Diff-in-Means Estimates \" + \"=\"*20)\n",
    "mean_DM_est, var_DM_est = ate_estimate_DM.mean(), ate_estimate_DM.var()\n",
    "print(f\"mean_DM_est: {mean_DM_est:.4f}\\nbias: {mean_DM_est - true_ATE:.4f}\\n\" + f\"var_DM_est: {var_DM_est:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
